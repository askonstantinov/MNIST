Цель ветки optuna - показать, как можно применить популярный инструмент автоматического подбора гиперпараметров OPTUNA в задаче MNIST.
Для демонстрации возможностей я подготовил скрипты MNIST-optuna.py и MNIST-optuna-adv.py
===
1) Благодаря MNIST-optuna.py , перебирая различные значения seed, средствами Optuna и была выявлена конфигурация ГП модели (1) для архитектуры, с которой ручной подбор ГП оптимизации (2) (правда, с некоторыми эвристиками в рамках системного подхода) позволил получить в итоге решение, которое показывает на тестовом датасете MNIST точность 99.72 % и здесь важно отметить, что Я НЕ ПРИМЕНЯЛ АУГМЕНТАЦИЮ (это можно сделать дополнительно).
Суть Optuna в том, чтобы применить smart подход (вместо грубых эвристик) и автоматизировать процесс поиска наилучшей комбинации гиперпараметров (ГП).
Поэтому важно представлять полный набор ГП.
Типы гиперпараметров:
ГП модели: включают параметры, определяющие архитектуру модели, например, количество скрытых слоев и нейронов;
для Optuna следует задавать модель явно (речь о 'скелете').
ГП оптимизации: к ним относятся параметры, управляющие процессом оптимизации, например, скорость обучения;
для Optuna можно использовать подгруженную модель, например, из onnx.
ГП регуляризации: имеют отношение, например, к коэффициенту dropout, а также настройкам L1 / L2 регуляризации.
Важное - L1 регуляризация (L1 penalty, Lasso) в torch не представлена
Важное - L2 регуляризация (L2 penalty, Ridge) в torch задается в настройках оптимизатора как weight_decay, поэтому целесообразно отнести к ГП оптимизации
Важное - dropout задается отдельным слоем, поэтому целесообразно отнести к ГП модели
2) В коде MNIST-optuna-adv.py продемонстрировано, как применить Оптуну к подбору количества полносвязных слоев с соответствующим количеством искусственных нейронов. После выполнения всех Trials запускается обучение с наилучшей конфигурацией.
Такой подход создает высокую вычислительную нагрузку на "железо" (у меня вычисления производились на cuda - MSI RTX 3060 8GB), поэтому пришлось сильно ограничить пространство поиска, применить "перебор по сетке из возможных значений" вместо поиска силами TPE самплера, а также настроить median pruner вместо более продвинутых прунеров.
По аналогии можно настроить подбор также для количества и расположения сверточных слоев и/или любых других слоев и ГП вообще. Ограничения - мощность "железа".
Из обнаруженных ограничений - dropout слой не всегда корректно отрабатывает в связке со слоем batchnorm.
3) Общие комментарии:
Осталась проблема с lock для seed (несмотря на мои попытки - новые запуски по-разному инициализируют стартовую конфигурацию Оптуны - для скрипта adv это выражается в разном начальном количестве подбираемых слоев).
Для улучшения результатов в задаче MNIST можно также добавить dropout после каждого слоя.
